# -*- coding: utf-8 -*-
"""194082P_ML_Assignment_01.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1C5lDS4C9I1-Z54S9dyIFGqSG1JqpblLW

### -------------------------------------------------------------------------------------------------------------------------------------------------------------
## Index: 194082P
## Name: Kudagodage N.E.
### -------------------------------------------------------------------------------------------------------------------------------------------------------------

### Solution for the Machine Learning and Pattern Recognition Assignment (CM 4370).
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats
from matplotlib import rcParams
from math import sqrt
from sklearn import linear_model
from sklearn.model_selection import cross_val_score, cross_val_predict
from sklearn.model_selection import KFold
from sklearn.model_selection import train_test_split
from sklearn import metrics
from sklearn.preprocessing import FunctionTransformer
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import PolynomialFeatures
from sklearn.decomposition import PCA
from sklearn.preprocessing import KBinsDiscretizer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OrdinalEncoder

rcParams['figure.figsize'] = 11.7, 8.27

"""##Load dataset

"""

from google.colab import drive
drive.mount('/content/gdrive')

file_path = '/content/gdrive/My Drive/Colab Notebooks/ML/Data/weatherHistory.csv'
df = pd.read_csv(file_path)

df

"""## a) Preprocess the dataset

Preprocessing stands as the initial step in the machine learning pipeline, where we enhance the reliability and consistency of our datasets prior to their utilization in machine learning models. This critical preprocessing phase serves to enhance model accuracies. Within the first section, I undertake fundamental preprocessing procedures to ensure that my dataset attains a refined and organized appearance. The preprocessing techniques that are included in this sections are,
   
       1. Handling missing values: It identifies and removes rows with missing values in the "Precip Type" column.
       2. Handling outliers: It identifies outliers in several numeric columns and applies a percentile-based method to remove or replace these outliers.
       3. Data transformation: Skewness of numeric features is checked, and appropriate transformations like logarithm or exponential are applied to make the data distribution more normal.
       4. Feature coding: Categorical columns like "Summary," "Precip Type," and "Daily Summary" are transformed into numerical values.
       5. Scaling: Numeric columns are scaled using Min-Max scaling to bring them within a specified range (usually [0, 1]).

##I.Handle Missing Values

*   Here the output is 'Precip Type'. That means the particular column has null values.
"""

df.isnull().sum()

df = df.dropna()
df = df.reset_index(drop=True)

df.isnull().sum()

df

column_names = ["Summary", "Precip Type", "Temperature (C)", "Apparent Temperature (C)", "Humidity", "Wind Speed (km/h)", "Wind Bearing (degrees)", "Visibility (km)", "Loud Cover", "Pressure (millibars)", "Daily Summary"]

y = df['Temperature (C)']
cleaned_df = df.copy().drop(['Temperature (C)', 'Formatted Date', 'Loud Cover'], axis=1)

cleaned_df

"""##II.Handle Outliers"""

df_train_num = cleaned_df.select_dtypes(include=[np.number])
df_train_num

class HandleOutliers:

    def __init__(self):
        pass

    def draw_boxplot(self, df, column_name=None, name="Before"):
        if column_name:
            p = sns.boxplot(data = df[column_name])
            p.title.set_text(name + "(" + column_name + ")")
        else:
            sns.boxplot(data = df)

    def percentileCheck(self, df, column_name=None):

        if column_name:
            min_val = df[column_name].quantile(0.05)
            max_val = df[column_name].quantile(0.95)

            outlier_df = df[(df[column_name] < min_val) | (df[column_name] > max_val)]

            return outlier_df
        else:
            print("Provide a valid Column Name")

    def percentileRemove(self, df, column_name=None):

        if column_name:
            min_val = df[column_name].quantile(0.05)
            max_val = df[column_name].quantile(0.95)

            df_2 = df.copy()

            df_2[column_name] = np.where(df_2[column_name] > max_val, max_val, df_2[column_name])
            df_2[column_name] = np.where(df_2[column_name] < min_val, min_val, df_2[column_name])

            return df_2
        else:
            print("Provide a valid Column Name")


outlier_object = HandleOutliers()

outlier_object.draw_boxplot(cleaned_df, 'Apparent Temperature (C)')
print(cleaned_df.shape)

outlier_df_per_1 = outlier_object.percentileCheck(cleaned_df, 'Apparent Temperature (C)')
print(outlier_df_per_1.shape)

cleaned_per_df = outlier_object.percentileRemove(cleaned_df, 'Apparent Temperature (C)')
print(cleaned_per_df.shape)

cleaned_df['Apparent Temperature (C)'] = cleaned_per_df['Apparent Temperature (C)']
outlier_object.draw_boxplot(cleaned_df, 'Apparent Temperature (C)', 'after')
print(cleaned_df.shape)

outlier_object.draw_boxplot(cleaned_df, 'Humidity')
print(cleaned_df.shape)

outlier_df_humidity = outlier_object.percentileCheck(cleaned_df, 'Humidity')
print(outlier_df_humidity.shape)

cleaned_per_df = outlier_object.percentileRemove(cleaned_df, 'Humidity')
print(cleaned_per_df.shape)

cleaned_df['Humidity'] = cleaned_per_df['Humidity']
outlier_object.draw_boxplot(cleaned_df, 'Humidity', 'after')
print(cleaned_df.shape)

outlier_object.draw_boxplot(cleaned_df, 'Wind Speed (km/h)')
print(cleaned_df.shape)

outlier_df_wind_speed = outlier_object.percentileCheck(cleaned_df, 'Wind Speed (km/h)')
print(outlier_df_wind_speed.shape)

cleaned_per_df = outlier_object.percentileRemove(cleaned_df, 'Wind Speed (km/h)')
print(cleaned_per_df.shape)

cleaned_df['Wind Speed (km/h)'] = cleaned_per_df['Wind Speed (km/h)']
outlier_object.draw_boxplot(cleaned_df, 'Wind Speed (km/h)', 'after')
print(cleaned_df.shape)

outlier_object.draw_boxplot(cleaned_df, 'Wind Bearing (degrees)')
print(cleaned_df.shape)

outlier_df_wind_bearing = outlier_object.percentileCheck(cleaned_df, 'Wind Bearing (degrees)')
print(outlier_df_wind_bearing.shape)

cleaned_per_df = outlier_object.percentileRemove(cleaned_df, 'Wind Bearing (degrees)')
print(cleaned_per_df.shape)

cleaned_df['Wind Bearing (degrees)'] = cleaned_per_df['Wind Bearing (degrees)']
outlier_object.draw_boxplot(cleaned_df, 'Wind Bearing (degrees)', 'after')
print(cleaned_df.shape)

outlier_object.draw_boxplot(cleaned_df, 'Visibility (km)')
print(cleaned_df.shape)

outlier_df_visibility = outlier_object.percentileCheck(cleaned_df, 'Visibility (km)')
print(outlier_df_visibility.shape)

cleaned_per_df = outlier_object.percentileRemove(cleaned_df, 'Visibility (km)')
print(cleaned_per_df.shape)

cleaned_df['Visibility (km)'] = cleaned_per_df['Visibility (km)']
outlier_object.draw_boxplot(cleaned_df, 'Visibility (km)', 'after')
print(cleaned_df.shape)

outlier_object.draw_boxplot(cleaned_df, 'Pressure (millibars)')
print(cleaned_df.shape)

outlier_df_pressure = outlier_object.percentileCheck(cleaned_df, 'Pressure (millibars)')
print(outlier_df_pressure.shape)

cleaned_per_df = outlier_object.percentileRemove(cleaned_df, 'Pressure (millibars)')
print(cleaned_per_df.shape)

cleaned_df['Pressure (millibars)'] = cleaned_per_df['Pressure (millibars)']
outlier_object.draw_boxplot(cleaned_df, 'Pressure (millibars)', 'after')
print(cleaned_df.shape)

"""##III. Apply Transformations

### Transformation Class
#### Transformation Class is defined to,

1. Plot graphs to identify the the shape of the distribution.

   Here I used two different plots to identify the distribution.
    
   - **QQ Plot**
     
   - **Histogram Plot**
   
    
2. Transform the columns that are needed to be transformed.

   Here I perform two main tasks,
   
   - Check the skewness
   - Perform transformation depending on the skewness of the distribution
   
   Ex:
       if skewness is greater than 1 then it's a right skewed distribution. So perform either np.log(Log transformation) or np.sqrt(Squre root transformation).
       
       if skewness is less than -1 then it's a left skewed distribution. So perform either np.reciprocal(Reciprocal transformation) or np.exponential(Power transformation).
"""

# Plotting Q-Q Plots and Histogram

class Transformation:

    def __init__(self):
        pass

    def plotQQ(self, df, column_name=None):

        if column_name:
            stats.probplot(df[column_name], dist="norm", plot=plt)
            plt.show()
        else:
            stats.probplot(df, dist="norm", plot=plt)
            plt.show()

    def plotHist(self, df, column_name=None):

        if column_name:
            df[column_name].hist()
        else:
            df.hist()

    def transform(self, df, transform_list):

        #skewness
        skew_values = np.array(df[transform_list].skew(axis=0, skipna=True, numeric_only = False))

        for i in range(len(skew_values)):
            value = skew_values[i]
            if (value > 1):
                logarithm_transformer = FunctionTransformer(np.log, validate=False, check_inverse=True)
                data_update = logarithm_transformer.transform(df[transform_list[i]])
                cleaned_df[transform_list[i]] = data_update

            elif (value < 1):
                exponential_transformer = FunctionTransformer(np.exp)
                data_update = exponential_transformer.transform(df[transform_list[i]])
                cleaned_df[transform_list[i]] = data_update

transformation_object = Transformation()

cleaned_df.skew(axis=0, skipna=True, numeric_only=True)

"""### Histogram & Q-Q Plot for Apparent Temperature (C)"""

transformation_object.plotQQ(cleaned_df, 'Apparent Temperature (C)')
transformation_object.plotHist(cleaned_df,'Apparent Temperature (C)')

"""### Histogram & Q-Q Plot for Humidity"""

transformation_object.plotQQ(cleaned_df, 'Humidity')
transformation_object.plotHist(cleaned_df,'Humidity')

"""### Histogram & Q-Q Plot for Wind Speed (km/h)

"""

transformation_object.plotQQ(cleaned_df, 'Wind Speed (km/h)')
transformation_object.plotHist(cleaned_df,'Wind Speed (km/h)')

"""### Histogram & Q-Q Plot for Wind Bearing (degrees)"""

transformation_object.plotQQ(cleaned_df, 'Wind Bearing (degrees)')
transformation_object.plotHist(cleaned_df,'Wind Bearing (degrees)')

"""### Histogram & Q-Q Plot for Visibility (km)"""

transformation_object.plotQQ(cleaned_df, 'Visibility (km)')
transformation_object.plotHist(cleaned_df,'Visibility (km)')

"""### Histogram & Q-Q Plot for Pressure (millibars)"""

transformation_object.plotQQ(cleaned_df, 'Pressure (millibars)')
transformation_object.plotHist(cleaned_df,'Pressure (millibars)')

"""##IV. Feature coding"""

distinct_values = cleaned_df['Summary'].value_counts()
distinct_values

mapping = {value: i for i, value in enumerate(cleaned_df['Summary'].unique())}

cleaned_df['Summary'] = cleaned_df['Summary'].map(mapping)

distinct_values = cleaned_df['Precip Type'].value_counts()
distinct_values

cleaned_df['Precip Type'] = cleaned_df['Precip Type'].map({'rain': 0, 'snow': 1})

distinct_values = cleaned_df['Daily Summary'].value_counts()
distinct_values

mapping = {value: i for i, value in enumerate(cleaned_df['Daily Summary'].unique())}

cleaned_df['Daily Summary'] = cleaned_df['Daily Summary'].map(mapping)

cleaned_df.head(10)

cleaned_df = cleaned_df.reset_index()

"""##V. Scale Function"""

def scale_numeric_columns(df):
    numeric_columns = df.select_dtypes(include=['number']).columns

    scaler = MinMaxScaler()

    df[numeric_columns] = scaler.fit_transform(df[numeric_columns])

    return df

scaled_df = scale_numeric_columns(cleaned_df)
scaled_df

"""##b) I. PCA

```
# This section applies dimensionality reduction using Principal Component Analysis on the preprocessed dataset. PCA reduces the dimensionality of the dataset while preserving as much variance as possible. This section calculates and visualizes explained variance ratios, cumulative explained variance, and eigenvalues to determine the optimal number of principal components.
```
"""

class DimensionReduction:

    def __init__(self):
        pass

    def applyPCA(self, df, n_components=None, column_names=None):
        pca = PCA(n_components=n_components)
        principalComponents = pca.fit_transform(df)
        principal_df = pd.DataFrame(data = principalComponents, columns = column_names)

        return principal_df, pca

    def plotCumVariance(self, pca):

        exp_var_pca = pca.explained_variance_ratio_
        cum_sum_eigenvalues = np.cumsum(exp_var_pca)

        plt.bar(range(0,len(exp_var_pca)), exp_var_pca, alpha=0.5, align='center', label='Individual explained variance')
        plt.step(range(0,len(cum_sum_eigenvalues)), cum_sum_eigenvalues, where='mid', label='Cumulative explained variance')
        plt.ylabel('Explained variance ratio')
        plt.xlabel('Principal component index')
        plt.legend(loc='best')
        plt.tight_layout()
        plt.show()

    def plotThreshold(self, pca):

        exp_var_pca = pca.explained_variance_ratio_
        cum_sum_eigenvalues = np.cumsum(exp_var_pca)

        compo = len(exp_var_pca)

        plt.ylim(0.0,1.1)
        plt.plot(np.arange(1, compo+1, step=1), cum_sum_eigenvalues, marker='o', linestyle='--', color='b')

        plt.xlabel('Number of Components')
        plt.xticks(np.arange(0, compo, step=1))
        plt.ylabel('Cumulative variance (%)')
        plt.title('The number of components needed to explain variance')

        plt.axhline(y=0.99, color='r', linestyle='-')
        plt.text(0.5, 0.85, '99% cut-off threshold', color = 'red', fontsize=16)

        plt.show()

    def screePlot(self, pca):
        plt.style.use("ggplot")
        plt.plot(pca.explained_variance_, marker='o')
        plt.xlabel("Eigenvalue number")
        plt.ylabel("Eigenvalue size")
        plt.title("Scree Plot")

dm_reduce = DimensionReduction()

principal_df, pca = dm_reduce.applyPCA(
    df=scaled_df,
    n_components=None
)

principal_df

dm_reduce.plotCumVariance(pca)
dm_reduce.plotThreshold(pca)
dm_reduce.screePlot(pca)

principal_df_2, pca = dm_reduce.applyPCA(
    df=scaled_df,
    n_components=0.99,
    column_names=None
)

principal_df_2

principal_df_final, pca = dm_reduce.applyPCA(
    df=scaled_df,
    n_components=9,
    column_names=["PCA_0", "PCA_1", "PCA_2", "PCA_3", "PCA_4", "PCA_5", "PCA_6", "PCA_7", "PCA_8"]
)

principal_df_final

"""##Model development

```
# In this section splits the dataset into training and testing sets, and it evaluates multiple machine learning models:
Linear Regression
Lasso Regression
Ridge Regression
For each model, the code performs 10-fold cross-validation, calculates the Root Mean Squared Error (RMSE), and measures the R-squared (R2) for model evaluation. It also includes data visualization to compare actual and predicted values.
```
"""

X_train, X_test, y_train, y_test = train_test_split(principal_df_final, y, test_size=0.2)
print(X_train.shape, y_train.shape)
print(X_test.shape, y_test.shape)

X_train = X_train.reset_index(drop=True)
X_test = X_test.reset_index(drop=True)
y_train = y_train.reset_index(drop=True)
y_test = y_test.reset_index(drop=True)

"""##Evaluation Functions"""

cv = KFold(n_splits=10, random_state=1, shuffle=True)

def rmse(y, pred):
    return sqrt(mean_squared_error(y, pred))

def r2_score(y, pred):
    return metrics.r2_score(y, pred)

def cross_validate_score(model, x, y):
    results = cross_val_score(model, x, y, cv=cv)
    return results

def cross_validate_predict(model, x, y):
    predictions = cross_val_predict(model, x, y, cv=cv)
    return predictions

"""##C) I. Linear Regression"""

linear_regressor = linear_model.LinearRegression()
linear_regressor.fit(X_train, y_train)

linear_scores = cross_validate_score(linear_regressor, X_train, y_train)
for i in range(len(linear_scores)):
    print("Fold %d Score: %.3f" %(i, linear_scores[i]))

print("10 Fold Cross Validation Score: ", np.mean(linear_scores))

linear_predictions = linear_regressor.predict(X_test)

print('Test RMSE: %.3f' % rmse(y_test, linear_predictions))
print("Test Accuracy:", r2_score(y_test, linear_predictions))

plt.plot(linear_predictions, '-', label = "Pred")
plt.plot(y_test, '-', label = "Actual")

plt.xlabel('x - axis')
plt.ylabel('y - axis')
plt.title('Actual and Pred Values')
plt.legend()

plt.show()

"""##II. Lasso"""

lasso_regressor = linear_model.Lasso(alpha=0.01)
lasso_regressor.fit(X_train, y_train)

lasso_scores = cross_validate_score(lasso_regressor, X_train, y_train)
for i in range(len(lasso_scores)):
    print("Fold %d Score: %.3f" %(i, lasso_scores[i]))

print("10 Fold Cross Validation Score: ", np.mean(lasso_scores))

lasso_predictions = lasso_regressor.predict(X_test)

print('Test RMSE: %.3f' % rmse(y_test, lasso_predictions))
print("Test Accuracy:", r2_score(y_test, lasso_predictions))

plt.plot(lasso_predictions, '-', label = "Pred")
plt.plot(y_test, '-', label = "Actual")

plt.xlabel('x - axis')
plt.ylabel('y - axis')
plt.title('Actual and Pred Values')
plt.legend()

plt.show()

"""##III. Ridge"""

ridge_regressor = linear_model.Ridge(alpha=0.01)
ridge_regressor.fit(X_train, y_train)

ridge_scores = cross_validate_score(ridge_regressor, X_train, y_train)
for i in range(len(ridge_scores)):
    print("Fold %d Score: %.3f" %(i, ridge_scores[i]))

print("10 Fold Cross Validation Score: ", np.mean(ridge_scores))

ridge_predictions = ridge_regressor.predict(X_test)

print('Test RMSE: %.3f' % rmse(y_test, ridge_predictions))
print("Test Accuracy:", r2_score(y_test, ridge_predictions))

plt.plot(ridge_predictions, '-', label = "Pred")
plt.plot(y_test, '-', label = "Actual")

plt.xlabel('x - axis')
plt.ylabel('y - axis')
plt.title('Actual and Pred Values')
plt.legend()

plt.show()